### 手动学深度学习v2
- https://zh-v2.d2l.ai/index.html (eng: Dive into Deep Learning: https://d2l.ai/index.html)
- lecture in bilibili: https://www.bilibili.com/video/BV1if4y147hS?spm_id_from=333.824.b_76696577626f785f7265706f7274.1
#### 2.1. Data Manipulation + lecture
- 0-d: scalar: a class
- 1-d: vector: a feature vector
- 2-d: matrix: a feature matrix of a sample
- 3-d: RGB image
- 4-d: a batch of RGB images
- 5-d: a batch of videos (+time)
- tensor class (ndarray in MXNet, Tensor in both PyTorch and TensorFlow) is similar to NumPy's ndarray with a few killer features
  - NumPy only supports CPU computation
  - GPU is well-supported to accelerate the computation; tensor class supports automatic differentiation
  - so, tensor class suitable for deep learning
##### in torch
- Basics
  - Unless otherwise specified, new tensors are stored in main memory and designated for CPU-based computation.
  - x = torch.arange(12)
  - x.shape
  - x.numel() # total number of elements
  - X = x.reshape(3, 4) or x.reshape(3,-1) # -1 means to automatically get it
  - torch.zeros((2, 3, 4)) or torch.ones((2, 3, 4)) or torch.randn(3, 4) # standard Gaussian (normal) distribution with a mean of 0 and a standard deviation of 1
  - e.g. torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
- Operations (**elementwise**)
  - x = torch.tensor([1.0, 2, 4, 8])
  - y = torch.tensor([2, 2, 2, 2])
  - x + y, x - y, x * y, x / y, x ** y
  - torch.exp(x)
  - torch.cat((X, Y), dim=0) # #row changes
  - torch.cat((X, Y), dim=1) # #col changes
  - X == Y => matrix of bools
  - X.sum()
- Broadcasting Mechanism 
  - a = torch.arange(3).reshape((3, 1))
  - b = torch.arange(2).reshape((1, 2))
  - tensor([[0],[1],[2]]), tensor([[0, 1]])
  - a + b
  - tensor([[0, 1], [1, 2], [2, 3]])
- Indexing and Slicing
  - access; assign
- Saving Memory
  - Y = Y + X will reassign memo to Y (don't need)
  - solution:
    - 1> Y[:] = Y + X # use ori memo of Y to store Y + X
    - 2> Y += X
- Conversion to Other Python Objects
  - Converting to a NumPy tensor (ndarray), or vice versa, is easy. The torch Tensor and numpy array will share their underlying memory locations, and changing one through an in-place operation will also change the other.
  - A = X.numpy(), B = torch.tensor(A) # type(A): numpy.ndarray, type(B): torch.Tensor
  - a = torch.tensor([3.5]); a, a.item(), float(a), int(a)
#### 2.2. Data Preprocessing
- handling missing data (pandas)
  - inputs = inputs.fillna(inputs.mean()) # numeric
  - inputs = pd.get_dummies(inputs, dummy_na=True) # category
- Conversion to the Tensor Format
  - X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
- tips
  - a = torch.arange(12)
  - b = a.reshape((3, 4))
  - b[:] = 2
  - then a became 2,2,2,...
