### Why deep not wide?
- Modularization 
  - simplify hard problem, share data
  - e.g. long hair guy data is small; use long/short hair then guy/girl
  - e.g. Speech
    - ')( Gaussian Mixture Model (GMM) -> shared distribution (state) -> partial shared distributions (state) (HMM-GMM) (problem: assuptionsis all models are trained independently, inefficient)
    - DNN (P(a to z|xi): all the states use the same DNN
      - Use parameters effectively (use lower layers first)
- Universality Theorem [sharing]
  - Fat (shallow network) is possible, but not effective  
  - Deep network: needs less parameters & less data (effective)
  - 类比：并联电路, 剪窗花，PCA，etc.
  - when data is less, if it's a sharing model => 有次序的崩坏
- Less engineering labor, but machine learns more
- Complex tasks
  - very different input, but the same label;
  - very similar input, but different lable.
  - needs more layers to do so, each layer distinguish a little more.
- Different perspect to explain why needs deep (refer to the lecture)
