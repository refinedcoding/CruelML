###### Adaptive LR and Adam optimization
- training stuck != critical point (i.m. GD==0)
  - different parameters GD could be very diff (so learning rate can't be one-size-fit-all)
  - different parameters need diff learning rate
    - basic: root mean square
    - advanced: weighted root mean square (RMSprop)
- Adam optimization
- diff learning rate
- Summary
###### Classification
- softmax
- minimizing cross-entropy is equivalent to maximizing likelihood
- Loss function: the number of times f gets incorrect results on training data. (not the sum(value diff))
- Change the loss function can change the difficulty of optimization (in the e.g., MSE might make it stuck at some area without using adam etc. techniques)
