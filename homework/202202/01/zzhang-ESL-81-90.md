# ESL: pp 81-90

(79-80)

 ##### Methods Using Derived Input Directions

è§£å†³è¾“å…¥å˜é‡ç›¸å…³æ€§å¾ˆå¼ºçš„é—®é¢˜ï¼š

- äº§ç”Ÿè¾ƒå°‘çš„ è¾“å…¥å˜é‡$X_j$çš„çº¿æ€§ç»„åˆ $Z_m$ï¼Œç”¨$Z_m$ ä½œä¸ºå›å½’çš„è¾“å…¥ã€‚

##### Principal Components Regression (PCR) ä¸»æˆåˆ†å›å½’

Derived: $z_m=Xv_m$, ç„¶ååœ¨ $z_1,\cdots,z_M$, $M\le p$ ä¸Šå›å½’ $y$ã€‚å…¶ä¸­ $z_m$ æ­£äº¤ã€‚

$\hat{y}=\bar{y}\mathbf{1} + \sum_{m=1}^{M}\hat\theta_{m}z_m$, $\hat\theta_m=\langle z_m,y\rangle/\langle z_m,z_m \rangle$

M = p å°±æ˜¯æœ€å°äºŒä¹˜ä¼°è®¡

M < p é™ç»´çš„regressionï¼Œä¸¢æ‰ p âˆ’ M ä¸ªæœ€å°çš„ç‰¹å¾å€¼åˆ†é‡ã€‚

 ##### Partial Least Squares (PLS) åæœ€å°äºŒä¹˜

å‡è®¾æ¯ä¸ª$x_j$ æ ‡å‡†åŒ–ä½¿å¾—å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1. 

$\hat\varphi_{1j}=\langle x_j,y\rangle$, $z_1=\sum_j \hat\varphi_{1j}x_j$. è¿™æ˜¯ç¬¬ä¸€åæœ€å°äºŒä¹˜æ–¹å‘ã€‚åœ¨æ¯ä¸ª $z_m$ çš„æ„é€ ä¸­ï¼Œè¾“å…¥å˜é‡é€šè¿‡åˆ¤æ–­å…¶åœ¨ $y$ä¸Šçš„å•å˜é‡å½±å“å¼ºåº¦æ¥åŠ æƒã€‚

ç®—æ³•æ­¥éª¤ m = 1, 2, ..., p

1. $z_m=\sum_{j=1}^p \hat\varphi_{mj}x_j^{(m-1)}$, $\hat\varphi_{mj}=\langle x_j^{(m-1)},y\rangle$
2. $\hat\theta_m=\langle z_m,y\rangle/\langle z_m,z_m\rangle$
3. $\hat{y}^{(m)}=\hat{y}^{(m-1)}+\hat\theta_mz_m$.
4. Orthogonalize each $x_j^{(m-1)}$ : $x_j^{(m)}=x_j^{(m-1)}-[\langle z_m,x_j^{m-1}\rangle / \langle z_m,z_m\rangle]z_m$

å¦‚æœè¾“å…¥çŸ©é˜µ $X$ æ˜¯æ­£äº¤çš„ï¼Œåˆ™åæœ€å°äºŒä¹˜ä¼šç»è¿‡ $m=1$ æ­¥æ‰¾åˆ°æœ€å°äºŒä¹˜ä¼°è®¡ã€‚å½“ $m>1$ æ—¶ï¼Œ$\hat\varphi_{mj}=0$.

åæœ€å°äºŒä¹˜å¯»æ‰¾**æœ‰é«˜æ–¹å·®**ä»¥åŠ**å’Œå“åº”å˜é‡æœ‰é«˜ç›¸å…³æ€§**çš„æ–¹å‘ï¼Œè€Œä¸ä¹‹ç›¸å¯¹çš„ä¸»æˆåˆ†åˆ†æå›å½’åªé‡è§†é«˜æ–¹å·®


---

##### Discussion: A comparison of the selection and shrinkage methods

å²­å›å½’å’Œ lasso çš„æƒ©ç½šå‚æ•°åœ¨ä¸€ä¸ªè¿ç»­çš„åŒºåŸŸå†…å˜åŒ–ï¼Œè€Œæœ€ä¼˜å­é›†ï¼ŒPLS å’Œ PCR åªè¦ä¸¤ä¸ªç¦»æ•£çš„æ­¥éª¤ä¾¿è¾¾åˆ°äº†æœ€å°äºŒä¹˜è§£ã€‚

å‚æ•°çš„ç›¸å…³ç³»æ•°ä¸ºæ­£æ—¶ï¼š

- ä»é›¶ç‚¹å¼€å§‹ï¼Œå²­å›å½’æ•´ä½“æ”¶ç¼©å‚æ•°ç›´åˆ°æœ€åæ”¶ç¼©åˆ°æœ€å°äºŒä¹˜ã€‚å°½ç®¡ PLS å’Œ PCR æ˜¯ç¦»æ•£çš„ä¸”æ›´åŠ æç«¯ï¼Œä½†å®ƒä»¬æ˜¾ç¤ºäº†ç±»ä¼¼å²­å›å½’çš„è¡Œä¸ºã€‚æœ€ä¼˜å­é›†è¶…å‡ºè§£ç„¶åå›æº¯ã€‚lasso çš„è¡Œä¸ºæ˜¯å…¶ä»–æ–¹æ³•çš„è¿‡æ¸¡ã€‚

å‚æ•°çš„ç›¸å…³ç³»æ•°ä¸ºè´Ÿæ—¶ï¼š

- PLS å’Œ PCR å¤§è‡´åœ°è·Ÿéšå²­å›å½’çš„è·¯å¾„ï¼Œè€Œæ‰€æœ‰çš„æ–¹æ³•éƒ½æ›´åŠ ç›¸ä¼¼ã€‚



Ridge å²­å›å½’ï¼šå¯¹æ‰€æœ‰æ–¹å‘éƒ½æœ‰æ”¶ç¼©ä½†åœ¨ä½æ–¹å·®æ–¹å‘æ”¶ç¼©ç¨‹åº¦æ›´å‰å®³ã€‚

PCR ä¸»æˆåˆ†å›å½’ï¼šå°† MM ä¸ªé«˜æ–¹å·®çš„æ–¹å‘å•ç‹¬å–å‡ºæ¥ï¼Œç„¶åä¸¢æ‰å‰©ä¸‹çš„ã€‚

PLS åæœ€å°äºŒä¹˜å›å½’ï¼šè¶‹å‘äºæ”¶ç¼©ä½æ–¹å·®çš„æ–¹å‘ï¼Œä½†æ˜¯å®é™…ä¸Šä¼šä½¿å¾—æŸäº›é«˜æ–¹å·®æ–¹å‘è†¨èƒ€ã€‚ä¸å¤ªç¨³å®šï¼Œå› æ­¤ç›¸æ¯”äºå²­å›å½’ä¼šæœ‰è¾ƒå¤§çš„é¢„æµ‹è¯¯å·®ã€‚



ğŸŒŸğŸŒŸ å¯¹äºæœ€å°åŒ–é¢„æµ‹è¯¯å·® prediction errorï¼Œridge å²­å›å½’ä¸€èˆ¬æ¯”å˜é‡å­é›†é€‰æ‹© subset selectionã€ä¸»æˆåˆ†å›å½’ PCR å’Œåæœ€å°äºŒä¹˜ PCR æ›´å¥½ã€‚ç„¶è€Œï¼Œç›¸å¯¹äºåä¸¤ç§æ–¹æ³•çš„æé«˜åªæ˜¯å¾ˆå°çš„ã€‚

æ€»çš„æ¥è¯´ï¼ŒPLSï¼ŒPCR ä»¥åŠå²­å›å½’è¶‹å‘äºè¡¨ç°ä¸€è‡´ï¼Œä½†å²­å›å½’æ›´å¥½ï¼Œæ”¶ç¼©å¾—å¾ˆå…‰æ»‘ï¼Œä¸åƒç¦»æ•£æ­¥éª¤ä¸­ä¸€æ ·ã€‚Lasso ä»‹äºå²­å›å½’å’Œæœ€ä¼˜å­é›†å›å½’ä¸­é—´ï¼Œå¹¶ä¸”æœ‰ä¸¤è€…çš„éƒ¨åˆ†æ€§è´¨ã€‚



#### Multiple Outcome shrinkage and selection

å¤šé‡è¾“å‡ºï¼š

$Y_k=f(X)+\epsilon_k$, $Y_\ell=f(X)+\epsilon_{\ell}$. æœ‰ç›¸åŒçš„æ¨¡å‹ç»“æ„ $f(X)$ => åº”è¯¥åˆå¹¶ $Y_k$ å’Œ $Y_\ell$ æ¥ä¼°è®¡å…±åŒçš„ $f$.

åˆå¹¶å“åº”å˜é‡ => **CCA (canonical correlation analysis) ** å…¸åˆ™ç›¸å…³åˆ†æ

- å¯»æ‰¾ $x_j$ çš„ä¸€åˆ—ä¸ç›¸å…³çš„çº¿æ€§ç»„åˆ $Xv_{m},m=1,\cdots,M$ ä»¥åŠå¯¹åº”çš„å“åº”å˜é‡ $y_k$ çš„ä¸ç›¸å…³çš„çº¿æ€§ç»„åˆ $Yu_m$ï¼Œä½¿å¾—ç›¸å…³ç³»æ•° $Corr^2(Yu_m,Xv_m)$ ç›¸ç»§æœ€å¤§ã€‚

å…·ä½“æ–¹æ³•å¦‚ reduced-rank regression ç•¥ã€‚

#### More on the lasso and related path algorithms

##### Incremental forward stagewise regression

- é€šè¿‡é‡å¤æ›´æ–°ä¸å½“å‰æ®‹å·®æœ€ç›¸å…³çš„å˜é‡çš„ç³»æ•°ï¼ˆä¹˜ä»¥ä¸€ä¸ªå°é‡ $\epsilon$ï¼‰å¾—åˆ°ç³»æ•°æ›²çº¿
- $\epsilon$ ç›¸å½“äº step size.

ç®—æ³•ç»†èŠ‚ï¼š

1. Intialization: normalize predictors. residual $r = y$.
2. find the predictor $x_j$ most correlated with residual $r$
3. Update $\beta_j\gets \beta_j+\delta_j$, $\delta_j=\epsilon\cdot \text{sign}[\langle x_j,r\rangle]$. $r\gets r - \delta_jx_j$.
4. é‡å¤2-3ï¼Œç›´åˆ° residual are uncorrelated with all the predictors

$\epsilon\to 0$ çš„æ—¶å€™å«åš **æ— ç©·å°çš„å‘å‰é€æ¸å›å½’ (infinitesimal forward stagewise regression)** ï¼Œå’ŒLASSOçš„è·¯å¾„å·®ä¸å¤šã€‚åœ¨éçº¿æ€§ã€è‡ªé€‚åº”æ–¹æ³•ä¸­æœ‰ç€å¾ˆé‡è¦çš„ä½œç”¨ï¼Œæ¯”å¦‚ boostingã€‚

- å…¨ $FS_0$ è·¯å¾„å¯ä»¥é€šè¿‡ LAR ç®—æ³•éå¸¸æœ‰æ•ˆåœ°è®¡ç®—å‡ºæ¥ã€‚

- $FS_0$ ç³»æ•°æ›²çº¿æ˜¯å¾®åˆ†æ–¹ç¨‹çš„ä¸€ä¸ªè§£ã€‚

- LASSOåœ¨é™ä½ç³»æ•°å‘é‡ $\beta$ çš„ $L_1$ èŒƒæ•°çš„ å•ä½æ®‹å·®å¹³æ–¹å’Œ å¢é•¿æ–¹é¢å®ç°äº†æœ€ä¼˜åŒ–ï¼›ä½† $FS_0$ åœ¨æ²¿ç€ç³»æ•°è·¯å¾„çš„ $L_1$ å¼§é•¿çš„å•ä½å¢é•¿æ˜¯æœ€ä¼˜çš„ï¼Œå› æ­¤å®ƒçš„ç³»æ•°æ›²çº¿ä¸ä¼šç»å¸¸æ”¹å˜æ–¹å‘ã€‚ï¼Ÿï¼Ÿï¼Ÿ

- $FS_0$ æ¯” LASSOçš„çº¦æŸæ›´å¼ºã€‚$FS_0$ å¯èƒ½åœ¨ $p\gg N$ æƒ…å½¢ä¸‹å¾ˆæœ‰ç”¨ï¼Œå®ƒçš„ç³»æ•°æ›²çº¿ä¼šæ›´åŠ çš„å…‰æ»‘ï¼Œå› æ­¤æ¯”LASSOæœ‰æ›´å°çš„æ–¹å·®ã€‚

  

##### Piecewise-linear path algorithms

ç•¥

##### The dantzig selector

$\min_{\beta}\|\beta\|_1$ subject to $\|X^T(y-X\beta)\|_{\infty} \le s$

å¯ä»¥ç­‰ä»·åœ°å†™æˆ

$\min_{\beta} \|X^T(y-X\beta)\|_\infty$ subject to $\|\beta\|_1\le t$.



$L_\infty$ èŒƒæ•°ï¼šè¯¥å‘é‡ä¸­ç»å¯¹å€¼æœ€å¤§çš„ç»„åˆ†

ç±»ä¼¼ LASSOï¼Œç”¨æ¢¯åº¦ç»å¯¹å€¼çš„æœ€å¤§å€¼æ›¿æ¢å¹³æ–¹è¯¯å·®æŸå¤±ã€‚å½“ $t$ å˜å¤§ï¼Œå¦‚æœ $N < p$ï¼Œåˆ™ä¸¤ä¸ªè¿‡ç¨‹éƒ½ä¼šå¾—åˆ°æœ€å°äºŒä¹˜è§£ã€‚å¦‚æœ $p\ge N$ï¼Œå®ƒä»¬éƒ½å¾—åˆ°æœ€å°çš„ $L_1$ èŒƒæ•°çš„æœ€å°äºŒä¹˜è§£ã€‚

DS å¯èƒ½å¾—åˆ°éå¸¸ä¸ç¨³å®šçš„ç³»æ•°æ›²çº¿ï¼Œä¸å¤ªå¥½ï¼Œç•¥ã€‚
