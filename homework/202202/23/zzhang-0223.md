## Logistic Regression and LDA: Review 1

#### Logistic Regression:

使用对数几率函数：

- $y=\frac{1}{1+e^{-(w^Tx+b)}}$

则有

- $\ln\frac{y}{1-y}=w^Tx+b$ 
  - $\frac{y}{1-y}$ 相当于是 odds 真是标记的几率，$\ln\frac{y}{1-y}$ 就是对数几率
  - 将$y$ 视为后验概率估计，既$\Pr(Y=1|X)$ 有 $\Pr(Y=1|X)=\frac{1}{1+e^{W^TX+b}}$

含义：

- 用线性回归模型的预测值 逼近分类任务真实标记的对数几率

优点：

- 直接对分类的概率进行建模，无需假设数据分布
- 对数几率函数任意阶可导凸函数。



极大似然法求解参数：

- 令 $P(Y=1|x)=p(x)$, $P(Y=0|x)=1-p(x)$ ，则似然函数为
  - $L(w) = \prod[p(x_i)]^{y_i}[1-p(x_i)]^{1-y_i}$
  - $$\ln L(w) &=& \sum[y_i\ln p(x_i) + (1-y_i)\ln (1-p(x_i))]\\
    &=&\sum[y_i\ln\frac{p(x_i)}{1-p(x_i)}+\ln (1-p(x_i))]\\
    &=&\sum[y_i (w\cdot x_i) - \ln(1+e^{w\cdot x_i})]$$

Loss function

$J(w)=-\frac{1}{N}L(w)$



正则化：

- L1：
  - 添加先验知识：$w$ 服从均值为0的拉普拉斯分布
  - $f(w|\mu,b)=\frac{1}{2b}\exp(-\frac{|w-\mu|}{b})$
- L2:
  - 添加先验知识：$w$ 服从均值为0的正态分布
  - $f(w|\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma}}\exp(-\frac{(w-\mu)^2}{2\sigma^2})$



问题：

1. 逻辑回归为什么不用MSE作为Loss function而用log loss
   - 用平方差损失函数非凸，很难找到全局最优解
     - 推导在[here](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c)
2. 逻辑回归和SVM的相同点和不同点
   - 相同点：
     - 监督学习，本质上都是找最佳分类超平面进行分类，都是判别式模型（不关心数据是怎么生成的，只关心数据之间的差别，用差别来分类）
   - 不同点：
     - LR是统计方法，SVM是几何方法
     - LR的损失函数是交叉熵，SVM的损失函数是HingeLoss
     - LR是参数模型，SVM是非参数模型：
       - 参数模型的前提是假设数据服从某一分布，该分布由一些参数确定
         - LR受数据分布影响，当样本不均衡时，需要先做平衡
       - 非参数模型对总体的分布不做任何假设，无法知道分布的形式。只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断
     - LR可以产生概率，SVM不能产生概率
     - LR不依赖样本间的距离，SVM依赖

3. 逻辑回归和朴素贝叶斯的相同点和不同点

   - 相同点：
     - 分类模型，当朴素贝叶斯的条件概率 $\Pr(X|Y=c_k)$ 服从高斯分布时，她计算得到的后验概率$\Pr(Y=1|X)$ 形式和LR相同。

   - 不同点：

     - LR是判别式模型 $p(y|x)$，朴素贝叶斯是生成式模型 $p(x,y)$

       - 判别式模型：估计条件概率，给定观测变量$x$和目标变量$y$的条件模型，由数据直接学习决策函数$y=f(x)$或者用条件概率分布$\Pr(y|x)$进行预测
       - 生成是模型：估计联合概率分布。首先建立样本的联合概率密度密度模型$\Pr(x,y)$，然后再得到后验概率$\Pr(y|x)$，然后进行分类。

     - 朴素贝叶斯假设各个特征权重相互独立。

       









