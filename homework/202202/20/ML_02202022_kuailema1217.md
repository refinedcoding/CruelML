### Word2Vec
- https://zhuanlan.zhihu.com/p/26306795
- one method of word embedding (word => math represent)
- f(word) -> word, but only care about the side effect of the model: parameters (embedding)
- hiden layer functions are linear, and after the training, the embedding vecter actually is the NN weights
- two methods:
  - 1> Skip-gram: f(word) -> context (words)
  - 2> CBOW: f(context(words)) -> word
- tricks to train Word2Vec
  - hierarchical softmax: to transfer N-category problem to log(N) times two-category problem
  - negative sampling: predict a subset of the category population
- todo:
  - Paper: word2vec Parameter Learning Explained
  - Zhihu: word2vec 相比之前的 Word Embedding 方法好在什么地方？
  - Two-tower model; DSSM
