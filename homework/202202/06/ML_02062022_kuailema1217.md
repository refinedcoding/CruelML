##### Logistic Regression
- if the posterior function we use is sigmoid(z(x))
- loss function: cross entropy
  - how close two bernoulli distributions are
  - computation: 对w算偏微分
  - Gradient descent is the same for Linear Regression and Logistic Regression!
  - Why couldn't use Square Error as loss function for Logistic Regression?
    - no matter when y == 0 or y == 1, when f(x) == 0 or f(x) == 1 (close or far from the target), the GD all == 0, which makes the learning hard.
- Descriminative v.s. Generative Models
  - e.g.s:
    - Logistic regression we did before: Descriminative
    - Gaussian distribution we did before: Generative
  - model function sets are the same: sigmoid(wx+b)
    - if in the Gaussian distribution we did before, we use shared covariance matrix
  - the parameters (w and b) got from two ways are different. Because in Generative Models, we have **assumptions**, e.g. Gaussian distribution, naive bayes (independent params assumption)
  - Descriminative (LR model) performance is better.
  - However, the benefit of generateive models:
    - with the assumption of prob distribution, less training data is needed (when training data is limited)
    - with the assumption of prob distribution, more robust to the noise (human assumption help with noise)
    - priors and class-dependent probabilities can be estimated from different sources. 
  - limitation of Logistic Regression
  - not linear-seperatable data
    - solutions: feature transformation
    - e.g. cascading LR (first LR's are doing feature transformation) <- actually is NN lol
