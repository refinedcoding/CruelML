### Different Models
#### Embedding + MLP (2016 Microsoft: Deep Crossing)
- Sparse Feature (e.g. sparse one-hot) is bad for NN: (From zhihu)
  - 1. only a few dimensions (not zero ones) get update in the training, not efficient
  - 2. dimension is large, computational expensive
- Structure: 5 layers
  - 对于类别特征，先利用 Embedding 层进行特征稠密化，再利用 Stacking 层连接其他特征，输入 MLP 的多层结构(ReLU)，最后用 Scoring 层(Sigmoid)预估结果。(refer to the code with TensorFlow)
#### Wide & Deep (2016 Google)
- Structure
  - Wide: 把输入层直接连接到输出层，中间没有做任何处理
  - Deep层: Embedding+MLP (same with above 5 layer structure)
    - findings:
    - Wide 部分只利用了两个特征的交叉：“已安装应用”和“当前曝光应用”。Wide 部分想学到的知识非常直观：“如果 A 所以 B”这样的简单规则。在 Google Play 的场景下，就是希望记住“如果用户已经安装了应用 A，是否会安装 B”这样的规则。
- Explanation
  - Wide: Memorization
    - 模型直接学习历史数据中物品或者特征的“共现频率”，并且把它们直接作为推荐依据的能力(co-occurance). 让模型记住大量的直接且重要的规则，这正是单层的线性模型所擅长的。
    - [use exploitation][记忆]
  - Deep: Generalization: 模型对于新鲜样本、以及从未出现过的特征组合的预测能力。
    - [good for exploration][泛化]
- TODO: Cross Features
  - e.g. Netflix&Pandora_imp <-> Pandora_conversion P71
  - cross production transformation (Boolean function) P74
