### AUC
#### 1. ROC Curve: Receiver Operating Characteristic
##### What
- A graph showing the performance of a classification model at all classification thresholds.
- x-axis: False Positive Rate - FP / (FP + TN) - e.g. the bad ones in served ads out of all bad ones (served bad ones + filtered-out bad ones)
- y-axis: True Positive Rate (recall 找得全) - TP / (TP + FN) - e.g. the good ones in served ads out of all good ones (served good ones + filtered-out good ones)
- at different classification thresholds:
  - the lower the threshold => both TP and FP increase => both FPR and TPR increase
  - the higher the threshold => both TP and FP decrease => both FPR and TPR decrease
  - same direction of FPR and TPR change => the curve monotonically increases
- Four corners and diagonal line
  - (0, 1) - FPR=0, TPR=1 - perfect classifier [so, the more top left the curve, the better classifier]
  - (1, 0) - FPR=1, TPR=0 - worst classifier
  - (0, 0) - FPR=0, TPR=0 - if threshold is very high, e.g. 1, all classified as negative (Positive is 0, so TPR == FPR == 0)
  - (1, 1) - FPR=1, TPR=1 - if threshold is very low, e.g. 0, all classified as positive (Negative is 0, so TPR == FPR == 0)
  - points on the diagonal line: half sample classified as positive / negative
- The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example with the classifier. The larger, the better.
##### How
- on each threshold (could use all predicted scores), compute the FPR and TPR, and draw the point on the plot
- connect each point
#### 2. PR Curve
##### Meaning
- A graph showing the performance of a classification model at all classification thresholds.
- x-axis: True Positive Rate (Recall 找得全) - TP / (TP + FN) - e.g. the good ones in served ads out of all good ones (served good ones + filtered-out good ones)
- y-axis: Precision 找的准 - TP / (TP + FP) - e.g. the good ones in served ads out of all served ads (served good ones + served bad ones)
- at different classification thresholds:
  - the lower the threshold => **ROUGHLY** Recall(TPR) increases and Precision decreases
  - the higher the threshold => **ROUGHLY** Precision decreases and Precision increases
  - diff direction of Recall and Precision change => the curve **ROUGHLY** decreases
- the more top right the curve, the better classifier
##### How to draw
- on each threshold (could use all predicted scores), compute the Recall and Precision, and draw the point on the plot
- connect each point
#### 3. AUC: Area Under Curve
##### PR AUC v.s. ROC AUC
- both are useful to have a closer look at classifier performance around specific threholds
- when the ratio of positive / negative data changes in the data set:
  - the shape of ROC curve usually stay similar
    - used to meature performance, when try different data sample, don't want to see the shape change
  - the shape of P-R curve changes a lot
    - when we want to see the performance of a classifier on a specific data set
      - e.g. cares more on positive points or negative points
    - when we care more about the positive data
      - e.g. 信用卡欺诈检测，更关注 precision 和 recall，提高二分类的 threshold 就能提高 precision，降低 threshold 就能提高 recall，这时便可观察 PR 曲线，得到最优的 threshold
#### More to read
- https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves
- https://wulc.me/2018/06/16/ROC%20%E6%9B%B2%E7%BA%BF%E4%B8%8E%20PR%20%E6%9B%B2%E7%BA%BF/
- https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf
