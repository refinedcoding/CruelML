# Book reading: Deep Learning
## Start from Part II. Deep Networks: Modern Practices
01/23/2022 P166-170
### Chapter 6. Deep Feedforward Networks
- also called: feedforward neural networks, multilayer perceptrons (MLPs)
- goal:  to approximate some function f∗
- )( rnn: when feedforward neural networks are extended to include feedback connections
- networks: DAG - chain structures - most commonly used structure of NN 
  - e.g.f3(f2(f1(x)))
  - f1 is 1st layer; f3 is output layer
  - the training data does not show the desired output for each of these layers, that's why these layers are called hidden layers.
- linear model limitation: the model cannot understand the interaction between any two input variables
  - direction: use a mapping φ, as providing a set of features describing x, or as providing a new representation for x 
  - how to choose the mapping φ:
    - 1> use a very generic φ, like a RBF kernel
      - based only on the principle of local smoothness and do not encode enough prior information to solve advanced problems.
    - 2> manually engineer φ, like feature engineering
      - requires decades of human effort for each separate task
    - 3> The strategy of deep learning is to learn φ
      - φ defining a hidden layer [after φ, still have parameters w that map from φ(x) to the desired output]
      - good: combined benefit form 1> and 2>
        - from 1>: using a very broad family φ(x;θ)
        - from 2>: human encode knowledge to design families φ(x; θ) that expect to perform well (rather than finding precisely the right function)
