### Hongyi Li DL Course
#### ML
- ML ~ looking for a function
- Tasks: Regression; Classification; Structure Learning
- Gradient Descent: Local minima doesn't truely cause the problem (不是GD真正的痛点) **(TBC)**
#### DL - re-understand NN
- Any piecewise linear curve = constant + sum of a set of hard sigmoid function
- To approximate a hard sigmoid function could use:
  - 1> sigmoid function   (c * sigmoid(b + wx))    (x is sum of all dimensions of xi's)
  - 2> sum of set of ReLU's
- So sum of a set of (approximated hard sigmoid function) sigmoid functions OR sum of RelU functions <=> one layer of NN
- Which means, any model could be approximate by a NN
- Why deep not wide? (as wide could construct any linear) **(TBC)**
- refer to: https://www.bilibili.com/video/BV1Wv411h7kN?p=3
