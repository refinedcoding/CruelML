###### Batch and Momentum
- Small batch size and Momentum help with escape critical points
- Batch
  - **Time-wise**: (**large batch size** win)
    - (No batch: long time for cooldown, but powerful; With batch: short time for cooldown, but noisy) - not right:
    - But actually, [for each update (batch)] larger batch size does NOT require longer time to compute gradient (**considering GPU parellal computation**) [under certain size]
    - So, actually, under certain batch size, smaller batch size => more batchs to run in one epcho => might longer time to finish one epcho
  - **performance-wise** (**small batch size** win)
    - Optimization: larger batch size performs worse than smaller batch size
      - as noisy one could help with stucking at critical points (diff batch gradient are different at the point)
    - Generalization: smaller batch is better on testing data
      - flat minima (good minima) v.s. sharp minima (bad minima)
      - a possible reason: smaller batch prevent from getting to sharp minima, which might responds to larger diff b/w training and testing Y.
  - batch size is a hyperparameter to decide
  - There are research to use large batch size with technique to solve generalization issue.
- Momentum (like in physics, a ball rolls down (based on gradient descent))
  - vanila GD: update the parameter towards the opposite direction of the gradient at present
  - GD with Momentum: update the parameter towards the movement of last step minus gradient at present
