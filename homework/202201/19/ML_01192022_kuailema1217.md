### Machine Learning-Powered Search Ranking of Airbnb Experiences
#### Experiments Scalse
|Scale	|experience	|cities(or places in 2018)|
| --- | --- | --- |
|November 2016|	500|	12|
|2017|	5k|	60|
|2018|	20k|	1000|
||like: ads items|	like: brands|
#### Experiments Details
|Model phases|ranking strategy|	algorithm|	#features	|data	|performance	|implementation	|demension	
| --- | --- | --- | --- | --- | --- | --- | --- |
|Step 0	|randomly	|-	|to collect data	|bookings from clicks							
|Step 1	|hard filters (loc, date) + score	|Offline GBDT (binary classification, logloss)	|25(exp f)	|50k	|+13% compared to rule based	|daily airflow job offline retrain the model, store ranks in prod machine, and serve subset after filter when query	|# ranked exps				
|Step 2	|same as above	|same as above	|50(exp f, user f)	|250k (4k exps)	|+7.9% compared to step 1	|same as above, but for logged-in users (user id as key), for logged out users (the same score value, 0 as key), limited to top active 1mil users	|# ranked exps x (min(1mil, # users) + 1)				
|Step 3	|same as above	|Online scoring (based on a infrastructure)	|90(exp f, user f, query f)	|2mil (16k exps)	|+5.1% compared to step 2	|model deployment: transform GBDT model file (json) to an internal Java GBDT structure, which is loaed in the search service app					
- More of Step 3:
  - online feature storage / update: Different signals were stored differently depending on their size, update frequency, etc.					
  - User Features  (due to their large size (hundreds of millions of user keys))  stored in an online key-value store and search server boxes can look them up when a user does the search [updated daily]					
  - Experience Features (not that large (tens of thousands of Experiences))  stored in memory of the search server boxes and read directly from there [updated daily]
  - Query Features are not stored at all, and they are just read as they come in from the front end					
#### Learnings & Inspiration
- 1. don't use absolute value features that change dramatically, like # bookings. Use #bookings / #views instead. (like: don't use #clicks, but use CTR)
- 2. Metrics: NDCG, the actual booked experience's position after ranked on score.
- 3. PDP: partial dependence plot => after model trained, know the relation b/w the feature and label
- 4. mutual features: users' currently booked homes (distance, avilability); historical clicks (category, time of the day)
- 5. mutual features: becareful of data leakage (uses should unknown feature data in training)
- 6. logged-in and logged-out users model seperate, as the model have a high dependency on user features.
- 7. query features: etc., language <=> served language in exp, demographic <=> diff country people liks cat stereotype
- 8. Business Rules [GOOD]
 - primary KPI: bookings
 - Secondory objectives: promote quality, Discovering and promoting potential new hits early, Enforcing diversity in the top 8 results, Optimize Search without Location for Clickability
 - Could improve Secondory objectives while keep overall (primary KPI) neutral
- 9. Monitoring & Explaining Rankings [GOOD]
 - dashboard 1: Very good tool for internal use - help with ML model improvement, avoid ML model manipulation e.g. price compitation
 - dashboard 2: Very good tool talk to hosts (explain their exp's rankings changes)
- 10. Future works (a lot) [GOOD]
