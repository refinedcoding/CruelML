# ESL: pp 61-70

Subset selection

- å¾—åˆ°ä¸€ä¸ªå¯è§£é‡Šçš„ã€é¢„æµ‹è¯¯å·®å¯èƒ½æ¯”å…¨æ¨¡å‹ä½
- ç¦»æ•£çš„è¿‡ç¨‹ï¼ˆå˜é‡ä¿ç•™æˆ–ä¸¢å¼ƒï¼‰=> high variance => æ²¡æœ‰é™ä½å…¨æ¨¡å‹çš„é¢„æµ‹è¯¯å·®



## Shrinkage methods



#### Ridge Regression

The ridge coefficients minimize a penalized residual sum of squares

$\hat{\beta}^{\text{ridge}}=\arg\min_{\beta}\{\sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j)^2+\lambda\sum_{j=1}^p \beta_j^2\}$

$\lambda$ å€¼è¶Šå¤§ï¼Œæ”¶ç¼©çš„ç¨‹åº¦è¶Šå¤§ï¼æ¯ä¸ªç³»æ•°éƒ½å‘é›¶æ”¶ç¼©



ç›¸å½“äºæ±‚è§£

$\text{minimize} \sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j)^2$

$\text{subject to}\sum_{j=1}^{p}\beta_j^2\le t$



å¯¹è¾“å…¥æŒ‰æ¯”ä¾‹è¿›è¡Œç¼©æ”¾æ—¶ï¼Œå²­å›å½’çš„è§£ä¸ç›¸ç­‰ï¼Œå› æ­¤æ±‚è§£ä¹‹å‰éœ€è¦å¯¹è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–ã€‚constraintä¸ $\beta_0$ æ— å…³ï¼Œæ‰€ä»¥å¯ä»¥å¯¹è¾“å…¥è¿›è¡Œä¸­å¿ƒåŒ–ã€‚ç”¨$\bar{y}=\frac{1}{N}\sum_{i=1}^Ny_i$ æ¥ä¼°è®¡ $\beta_0$ã€‚å‡è®¾ä¸­å¿ƒåŒ–å·²ç»å®Œæˆï¼Œè¾“å…¥çŸ©é˜µ $X$ æœ‰påˆ—ã€‚



å†™æˆçŸ©é˜µå½¢å¼ï¼š$\text{RSS}(\lambda)=(y-X\beta)^T(y-X\beta)+\lambda\beta^T\beta$

å¾—åˆ°è§£ $\hat{\beta}^{\text{ridge}}=(X^TX+\lambda I)^{-1}X^Ty$

å²­å›å½’çš„è§£ä»æ˜¯ $y$ çš„çº¿æ€§å‡½æ•°ã€‚åœ¨æ±‚é€†ä¹‹å‰å¯¹è§’å…ƒä¸ŠåŠ ä¸Š $\lambda$ï¼Œå³ä½¿ $X^TX$ä¸æ˜¯full rankï¼ŒåŠ ä¸Šä¹‹åä¹Ÿæ˜¯éå¥‡å¼‚çš„ã€‚



---

ğŸŒŸğŸŒŸ

å½“ç»™å®šä¸€ä¸ªåˆé€‚çš„å…ˆéªŒåˆ†å¸ƒï¼Œå²­å›å½’ä¹Ÿå¯ä»¥ä»åéªŒåˆ†å¸ƒçš„å‡å€¼æˆ–ä¼—æ•°å¾—åˆ°ã€‚

å‡è®¾ $y_i\sim N(\beta_0+x_i^T\beta, \sigma^2)$. å‚æ•° $\beta_j\sim N(0,\tau^2)$ ä¸”ç›¸äº’ç‹¬ç«‹ã€‚å½“$\tau^2$ å’Œ $\sigma^2$ çš„å€¼å·²çŸ¥æ—¶ï¼Œ $\beta$ åéªŒåˆ†å¸ƒå¯†åº¦å‡½æ•°çš„ç»å¯¹å€¼ï¼ˆçš„è´Ÿæ•°ï¼‰å’Œ $\{\sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j)^2+\lambda\sum_{j=1}^p \beta_j^2\}$ æˆæ¯”ä¾‹ ä¸” $\lambda=\frac{\sigma^2}{\tau^2}$.

å› æ­¤å²­å›å½’ä¼°è®¡æ˜¯åéªŒåˆ†å¸ƒçš„ä¼—æ•°ï¼›åˆå› åˆ†å¸ƒä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œåˆ™ä¹Ÿæ˜¯åéªŒåˆ†å¸ƒçš„å‡å€¼ï¼

---

åˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆ$$X=UDV^T$$ï¼‰æˆ‘ä»¬å¯ä»¥æŠŠæœ€å°äºŒä¹˜æ‹Ÿåˆå‘é‡å†™æˆ

$X\hat\beta^{\text{ls}}=X(X^TX)^{-1}X^Ty=UU^Ty$. $U^Ty$ æ˜¯$y$ æ­£äº¤åŸº $U$ä¸‹çš„åæ ‡ã€‚

$\hat\beta=R^{-1}Q^Ty$, $\hat{y}=QQ^Ty$

$Q$ å’Œ $U$ æ˜¯ $X$ åˆ—ç©ºé—´çš„ä¸¤ä¸ªä¸åŒçš„æ­£äº¤åŸºã€‚

å²­å›å½’çš„è§£ä¸º 

- $X\hat{\beta}^{\text{ridge}}=X(X^TX+\lambda I)^{-1}X^Ty = UD(D^2+\lambda I)^{-1}DU^Ty=\sum_{j=1}^p u_j\frac{d_j^2}{d_j^2+\lambda}u_j^Ty$

- $u_j$  æ˜¯ $U$ çš„åˆ—å‘é‡
- $\frac{d_j^2}{d_j^2+\lambda}<1$.å²­å›å½’è®¡ç®— $y$ å…³äº æ­£äº¤åŸº $U$ çš„åæ ‡ï¼Œé€šè¿‡å› å­ $\frac{d_j^2}{d_j^2+\lambda}<1$ æ”¶ç¼©åæ ‡ã€‚æ›´å°çš„ $d_j^2$ ä¼šæ›´å¤§ç¨‹åº¦ä¸Šæ”¶ç¼©åŸºå‘é‡çš„åæ ‡ã€‚

- å› æ­¤è¶Šå°çš„å¥‡å¼‚å€¼ $d_j$ å¯¹åº” $X$ åˆ—ç©ºé—´ä¸­æ–¹å·®è¶Šå°çš„æ–¹å‘ï¼Œå¹¶ä¸”å²­å›å½’åœ¨è¿™äº›æ–¹å‘ä¸Šæ”¶ç¼©å¾—æœ€å‰å®³ã€‚

$df(\lambda)=tr[X(X^TX+\lambda I)^{-1}X^T]=tr(H_\lambda)=\sum_{j=1}^p\frac{d_j^2}{d_j^2+\lambda}$

- This monotone decreasing function of $\lambda$ is the *effective degress of freedom* of  the ridge regression fit. å²­å›å½’æ‹Ÿåˆçš„ **æœ‰æ•ˆè‡ªç”±åº¦ (effective degrees of freedom)**
- å½“ $\lambda = 0$, $df(\lambda)=p$. å½“ $\lambda \to \infty$ï¼Œ$df(\lambda)\to 0$



### Lasso

Lasso estimate å®šä¹‰å¦‚ä¸‹

$\hat{\beta}^{\text{lasso}} = \arg\min_{\beta} \sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j)^2$

$\text{subject to}\sum_{j=1}^{p}|\beta_j|\le t$

=> (Lagrangian form)

$\hat{\beta}^{\text{lasso}}=\arg\min_{\beta}\{\sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j)^2+\lambda\sum_{j=1}^p |\beta_j|\}$

è®¡ç®— lasso çš„è§£æ˜¯ä¸€ä¸ªäºŒæ¬¡è§„åˆ’é—®é¢˜ã€‚ä»¤ $t$ å……åˆ†å°ä¼šé€ æˆä¸€äº›å‚æ•°æ°æ°ç­‰äº0ï¼Œå› æ­¤Lassoå®Œæˆä¸€ä¸ªæ¸©å’Œçš„è¿ç»­å­é›†é€‰æ‹©ã€‚

