## 16 PyTorch 神经网络基础【动手学深度学习v2】

## **神经网络基础**

## 模型构造

定义了一个nn.module的子类

nn.ReLU() 激活函数

实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层。

init（定义好有哪些层）、foward（前向运算如何计算）函数中做大量自定义

反向计算是对forward函数的自动求导

## 参数管理

把里面每一层的权重拿出来

parameter 可以优化的参数

named_parameters() 把网络里的参数全部拿出来

print可以看出网络是长什么样子

## 内置初始化

def init_normal(m) :

if type(m) == nn.Linear:

nn.init.normal_(m.weight, mean = 0, std = 0.01) #_替换函数

nn

net.apply(init_normal) 对net里的所有的module，作为参数传入 for loop遍历一遍

shared 参数绑定，在不同网络间共享权重。

## 自定义层

## 读写文件

torch.save(x,’x-file’)

torch.save([x,y], ’x-file’)

torch.load(’x-file’)

torch.save(net.state_dict(), ‘mlp.params’)

备份模型 声明参数

clone = MLP()

clone.load_state_dict(torch.load(”mlp.params”))

clone.eval()

Y_clone = clone(X)

Y_clone == Y

查询张量所在设备

x = torch.tensor([1, 2, 3])

x.device

**17 使用和购买 GPU【动手学深度学习v2】**

存储在GPU

X = torch.ones(2, 3, device=try_gpu())

同一个GPU上，可作计算。

 net = nn.Sequential(nn.Linear(3, 1))

net = net.to(device=try_gpu())

GPU:显存，计算能力，价格

买能够承受的最好的

注意降温

保证模型的泛化性
