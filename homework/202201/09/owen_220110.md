## Note on D2L

https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html

### Implementation 

- A simple RNN language model consists of input encoding, RNN modeling, and output generation.
  - tokenization (one-hot or WordPiece)
- RNN models need state initialization for training, though random sampling and sequential partitioning use different ways.
  - When using sequential partitioning, we need to detach the gradient to reduce computational cost.
- A warm-up period allows a model to update itself
  - obtain a better hidden state than its initialized value) before making any prediction.
- Gradient clipping prevents gradient explosion, but it cannot fix vanishing gradients.

### Gradient vanishing

- An RNN that process a sequence data with the size of 10,000 time steps, has 10,000 deep layers which is very hard to optimize.
- Suppose we are working with language modeling problem and there are two sequences that model tries to learn:
    - "The **cat**, which already ate ..., **was** full"
    - "The **cats**, which already ate ..., **were** full"
    - Dots represent many words in between.
- What we need to learn here that "was" came with "cat" and that "were" came with "cats". The naive RNN is not very good at capturing very long-term dependencies like this.
- *Vanishing gradients* problem tends to be the bigger problem with RNNs than the *exploding gradients* problem. We will discuss how to solve it in next sections.
- Exploding gradients can be easily seen when your weight values become `NaN`. So one of the ways solve exploding gradient is to apply **gradient clipping** means if your gradient is more than some threshold 
- Re-scale some of your gradient vector so that is not too big. So there are cliped according to some maximum value.
